sbatch -N 128 -p normal -t 12:00:00 -o reproducibility-scripts/sft/plw_ablations/ademamix-plw-2025-08-31-11-58/out/plw-ablations/Apertus70B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs1024-lr2e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left.out -e reproducibility-scripts/sft/plw_ablations/ademamix-plw-2025-08-31-11-58/out/plw-ablations/Apertus70B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs1024-lr2e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left.err ./cscs-shared-submit-scripts/recursive-unattended-accelerate.sh -m swiss_alignment.train_sft dataset=apertus-sft-mixture-8e-ln model=apertus-70b model_args.model_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus70B-tokens15T-longcontext64k tokenizer_args.tokenizer_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus70B-tokens15T-longcontext64k trainer=ln-plw accelerate_config=src/swiss_alignment/configs/accelerate/ds-zero3.yaml plw_args.prompt_loss_weight=0.05 plw_args.sequence_level_loss=true training_args.gradient_accumulation_steps=1 training_args.per_device_train_batch_size=2 training_args.optim=ademamix training_args.learning_rate=2e-06 training_args.max_grad_norm=1.0 tokenizer_args.chat_template_name=apertus tokenizer_args.model_eos_token_id=68 tokenizer_args.padding_side=left training_args.num_train_epochs=1 artifacts_subdir=shared job_subdir=plw-ablations/Apertus70B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs1024-lr2e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left wandb.run_name=plw-ablations/Apertus70B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs1024-lr2e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left wandb.tags=[prod,ln-plw,default,plw-ablations] resuming.resume=True global_batch_size=1024 num_nodes=128 
# sbatch -N 64 -p normal -t 12:00:00 -o reproducibility-scripts/sft/plw_ablations/ademamix-plw-2025-08-31-11-58/out/plw-ablations/Apertus8B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left.out -e reproducibility-scripts/sft/plw_ablations/ademamix-plw-2025-08-31-11-58/out/plw-ablations/Apertus8B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left.err ./cscs-shared-submit-scripts/recursive-unattended-accelerate.sh -m swiss_alignment.train_sft dataset=apertus-sft-mixture-8e-ln model=apertus-8b model_args.model_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus8B-tokens15T-longcontext64k tokenizer_args.tokenizer_name_or_path=/capstor/store/cscs/swissai/infra01/pretrain-checkpoints/apertus/Apertus8B-tokens15T-longcontext64k trainer=ln-plw accelerate_config=src/swiss_alignment/configs/accelerate/ds-zero2.yaml plw_args.prompt_loss_weight=0.05 plw_args.sequence_level_loss=true training_args.gradient_accumulation_steps=1 training_args.per_device_train_batch_size=2 training_args.optim=ademamix training_args.learning_rate=5e-06 training_args.max_grad_norm=1.0 tokenizer_args.chat_template_name=apertus tokenizer_args.model_eos_token_id=68 tokenizer_args.padding_side=left training_args.num_train_epochs=1 artifacts_subdir=shared job_subdir=plw-ablations/Apertus8B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left wandb.run_name=plw-ablations/Apertus8B-tokens15T-longcontext64k-apertus-sft-mixture-8e-ln-plw0.05-bs512-lr5e-06-maxgnorm1.0-epochs1-ademamix-apertus-pad-left wandb.tags=[prod,ln-plw,default,plw-ablations] resuming.resume=True global_batch_size=512 num_nodes=64 

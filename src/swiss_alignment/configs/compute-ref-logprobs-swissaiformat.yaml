# An example config file for an experiment.
# Keep this, it's used as an example to run the code after a user installs the project.

defaults:
  # Common setup.
  - setup
  # This file.
  - _self_
  # Model (from the configs/model/ directory).
  - model: mistral
  # Dataset (from the configs/dataset/ directory).
  - dataset: magpieair-armorm


######################################################################

model_args:
  model_name_or_path: ${outputs_dir}/shared/train_sft/sft-chosen/llama-nosft-magpieair-armorm/checkpoints/861b236c6e408fff

split: train # train or eval
partition_start_idx: 0 # (node level) The start index of the partition to generate completions for.
partition_end_idx: 0
subpartition_number: 0 # (gpu level) subsection in the partition to process
batch_size: 16
max_seq_len : 4096
save_interval: 256 # Saving every save_interval prompt (should be multiple of batch_size)
num_gpus_per_node: 4

job_subdir_prefix: null
job_subdir: ${job_subdir_prefix}/${subpartition_number}

seed: 42

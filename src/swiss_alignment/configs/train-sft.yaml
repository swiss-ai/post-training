defaults:
  - setup
  - _self_
  # Model (from the configs/model/ directory).
  - model: apertus-8b
  # Dataset (from the configs/dataset/ directory).
  - dataset: tulu-3-sft-mixture-split
  # Optional preset for training parameters.
  - optional preset: none

######################################################################

wandb:
  project: swiss-alignment-sft

dataset_args:
  debug_oom: false # Reorder the dataset with largest batch first to test OOM
  debug_subsample:
    train: 0
    eval: 0
  shuffle: true

script_args:
  dataset_name: ${dataset_args.dataset_name}
  dataset_train_split: ${dataset_args.train_split.name}
  dataset_test_split: ${dataset_args.eval_split.name}

accelerate_config: "src/swiss_alignment/configs/accelerate/ds-zero1.yaml"
trainer: plw # can only take values: sft, plw, ln-plw, irl
plw_args:
  prompt_loss_weight: 0.0

training_args:
  bf16: true
  max_seq_length: 4096
  max_grad_norm: 1e6 # Effectively no clipping
  learning_rate: 5.0e-06
  lr_scheduler_type: linear
  warmup_ratio: 0.03
  weight_decay: 0.0
  num_train_epochs: 2
  packing: false
  dataloader_drop_last: true
  remove_unused_columns: false # required for custom data collator
  per_device_train_batch_size: 2 # note, this is set up for 4 GPUs
  gradient_accumulation_steps: 4 # effective batch size 128 with 4 nodes
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false # https://pytorch.org/docs/stable/checkpoint.html, https://github.com/huggingface/trl/issues/1348
  average_tokens_across_devices: true # More accurate loss.
  logging_first_step: true
  logging_steps: 1
  per_device_eval_batch_size: 2
  eval_strategy: steps # use "no" for no evaluation
  eval_steps: 100
  eval_on_start: false
  save_strategy: epoch
  save_steps: 1000
  report_to: wandb
  seed: ${seed}
  dataset_num_proc: 8
  dataloader_num_workers: 8

seed: 42

resuming:
  resume: false
  exclude_keys:
    - run_dir
    - artifacts_dir
    - artifacts_subdir
    - input_artifacts_subdir
    - output_artifacts_subdir
    - job_subdir
    - resuming_dir
    - wandb
    - resuming.exclude_keys
    - training_args.logging_dir

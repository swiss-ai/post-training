# @package _global_

model_args:
  torch_dtype: bfloat16
  model_name_or_path: mistralai/Mistral-7B-Instruct-v0.2
  attn_implementation: flash_attention_2
  trust_remote_code: true
  use_peft: false
  lora_task_type: CAUSAL_LM
  lora_r: 64
  lora_alpha: 32
  lora_dropout: 0.1 # Not clear why we want lora dropout
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

tokenizer_args:
  tokenizer_name_or_path: mistralai/Mistral-7B-Instruct-v0.2
  chat_template_name: null
  trust_remote_code: true
  padding_side: right
  add_bos: false
  model_pad_token_id: 0  # <unk>
  model_eos_token_id: null # keep default.

ref_completion_generation:
  # sources
  # https://github.com/search?q=repo%3Amistralai%2Fmistral-inference%20temperature&type=code
  # https://openrouter.ai/mistralai/mistral-7b-instruct/parameters
  temperature: 0.7
  top_p: 1
  extra_stop_token_id: null
